# Google Cloud Configuration
# Your Google Cloud Project ID
PROJECT_ID=your-project-id

# Your Google Cloud Project Location (e.g., us-central1)
LOCATION=us-central1

# Model Configuration
# The model to use.
# For Vertex AI (default): Gemini model ID, e.g. gemini-2.0-flash-exp, gemini-1.5-pro
# For LiteLLM backend: LiteLLM model string, e.g.
#   - ollama_chat/mistral-small3.1
#   - ollama_chat/llama3.2
#   - openai/gpt-4o-mini
MODEL_NAME=gemini-2.0-flash-exp

# Backend selection
# vertex_ai (default): use Vertex AI Gemini with model=f"vertex_ai/{MODEL_NAME}"
# litellm: use LiteLLM and pass MODEL_NAME directly to LiteLLM
# MODEL_BACKEND=vertex_ai

# Optional: API Keys
# If using Gemini API instead of Vertex AI
# GOOGLE_API_KEY=your-api-key-here

# Optional: LiteLLM / local model configuration
# --- Ollama local models ---
# OLLAMA_API_BASE=http://localhost:11434
#
# --- OpenAI-compatible local endpoints (e.g. vLLM, Ollama openai provider) ---
# OPENAI_API_BASE=http://localhost:11434/v1
# OPENAI_API_KEY=anything

# Optional: Additional Configuration
# Add any additional environment variables your agent needs here
